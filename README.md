# About
Inspired by [this repo](https://github.com/aleju/papers) and [ML Writing Month](https://docs.google.com/document/d/15o6m0I8g6O607mk5YPTh33Lu_aQYo7SpHhNSbLPQpWQ/mobilebasic?from=groupmessage#?utm_source=wechat_session&utm_medium=social&utm_oi=624560843380101120). Questions and discussions are most welcome!

[Lil-log](https://lilianweng.github.io/lil-log/) is the best blog I have ever read!

# Papers

## Survey
1. `TNNLS 2019` [Adversarial Examples: Attacks and Defenses for Deep Learning](https://ieeexplore.ieee.org/document/8611298)
2. `IEEE ACCESS 2018` [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://ieeexplore.ieee.org/document/8294186)
3. `2019` [Adversarial Attacks and Defenses in Images, Graphs and Text: A Review](https://arxiv.org/pdf/1909.08072)
4. `2019` [A Study of Black Box Adversarial Attacks in Computer Vision](https://arxiv.org/pdf/1912.01667)
5. `2019` [Adversarial Examples in Modern Machine Learning: A Review](https://arxiv.org/pdf/1911.05268)
6. `2020` [Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey](https://arxiv.org/abs/2007.00753)
7. `2020` [Knowledge Distillation and Student-Teacher Learning for Visual Intelligence\ A Review and New Outlooks](https://arxiv.org/pdf/2004.05937)
8. `2019` [Adversarial attack and defense in reinforcement learning-from AI security view](https://arxiv.org/pdf/1901.06796)
9. `2020` [A Survey of Privacy Attacks in Machine Learning](https://arxiv.org/abs/2007.07646)
10. `2020` [Learning from Noisy Labels with Deep Neural Networks: A Survey](https://arxiv.org/abs/2007.08199)
11. `2020` [Optimization for Deep Learning: An Overview](https://link.springer.com/epdf/10.1007/s40305-020-00309-6?sharing_token=Xv0f6yBzgc1QnNAUbQ9pufe4RwlQNchNByi7wbcMAY56wZ54Vxigc8CL-kHvhiYpSthXAu14ZSiMmkrVuqUSJUCRoWymQqZbEnVDQvz2sEBOiX8dkkGxS7bI7irClme0cEKnUtpyPIJONJQQDAiWTskwNws64eAd2xKnqi3nYOY%3D)
12. `2020` [Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review](https://arxiv.org/abs/2007.10760)
13. `2020` [Learning from Noisy Labels with Deep Neural Networks: A Survey](https://arxiv.org/pdf/2007.08199)
14. `2020` [Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective](https://arxiv.org/pdf/2009.03728)
15. `2020` [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
16. `2019` [A Survey of Black-Box Adversarial Attacks on Computer Vision Models](https://arxiv.org/abs/1912.01667)
17. `2020` [Backdoor Learning: A Survey](https://arxiv.org/abs/2007.08745)
18. `2020` [Transformers in Vision: A Survey](https://arxiv.org/abs/2101.01169)
19. `2020` [A Survey on Neural Network Interpretability](https://arxiv.org/abs/2012.14261)
20. `2020`[A Survey of Privacy Attacks in Machine Learning](https://arxiv.org/abs/2007.07646)
21. `2020` [Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses](https://arxiv.org/pdf/2012.10544.pdf)

## Attack
### 2013
1. `ICLR` [Evasion Attacks against Machine Learning at Test Time](./2013/Evasion_attacks_against_machine_learning_at_test_time.md)


### 2014
1. `ICLR` [Intriguing properties of neural networks](./2014/Intriguing_properties_of_neural_networks.md)
2. `ARXIV` [Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization]


### 2015
1. `ICLR` [Explaining and Harnessing Adversarial Examples](./2015/Explaining_and_Harnessing_Adversarial_Examples.md)


### 2016
1. `EuroS&P` [The limitations of deep learning in adversarial settings](./2016/The_limitations_of_deep_learning_in_adversarial_settings.md)
2. `CVPR` [Deepfool](./2016/DeepFool.md)
3. `SP` [C&W Towards evaluating the robustness of neural networks](./2016/Toward_evaluating_the_robustness_of_neural_networks.md)
4. `Arxiv` [Transferability in machine learning: from phenomena to black-box attacks using adversarial samples](./2016/Transferability_in_machine_learning.md)
5. `NIPS` [Adversarial Images for Variational Autoencoders]
6. `ARXIV` [A boundary tilting persepective on the phenomenon of adversarial examples]
7. `ARXIV` [Adversarial examples in the physical world]


### 2017
1. `ICLR` [Delving into Transferable Adversarial Examples and Black-box Attacks](./2017/Delving_into_Transferable_Adversarial_Examples_and_Black-box_Attacks.md)
2. `CVPR` [Universal Adversarial Perturbations](./2017/Universal_Adversarial_Perturbations.md)
3. `ICCV` [Adversarial Examples for Semantic Segmentation and Object Detection](./2017/Adversarial_Examples_for_Semantic_Segmentation_and_Object_Detection.md)
4. `ARXIV` [Adversarial Examples that Fool Detectors](./2017/Adversarial_Examples_that_Fool_Detectors.md)
5. `CVPR` [A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection](./2017/A-Fast-RCNN_Hard_Positive_Generation_via_Adversary_for_Object_Detection.md)
6. `ICCV` [Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics](./2017/Adversarial_Examples_Detection_in_Deep_Networks_with_Convolutional_Filter_Statistics.md)
7. `AIS` [Adversarial examples are not easily detected: Bypassing ten detection methods]
8. `ICCV` `UNIVERSAL` [Universal Adversarial Perturbations Against Semantic Image Segmentation]
9. `ICLR` [Adversarial Machine Learning at Scale]
10. `ARXIV` [The space of transferable adversarial examples]
11. `ARXIV` [Adversarial attacks on neural network policies]


### 2018
1. `ICLR` [Generating Natural Adversarial Examples](./2018/Generating_Natural_Adversarial_Examples.md)
2. `NeurlPS` [Constructing Unrestricted Adversarial Examples with Generative Models](./2018/Constructing_Unrestricted_Adversarial_Examples_with_Generative_Models.md)
3.  `IJCAI` [Generating Adversarial Examples with Adversarial Networks](./2018/Generating_Adversarial_Examples_with_Adversarial_Networks.md)
4.  `CVPR` [Generative Adversarial Perturbations](./2018/Generative_Adversarial_Perturbations.md)
5.  `AAAI` [Learning to Attack: Adversarial transformation networks](./2017/Adversarial_transformation_networks_Learning_to_generate_adversarial_examples.md)
6.  `S&P` [Learning Universal Adversarial Perturbations with Generative Models](./2018/Learning_Universal_Adversarial_Perturbations_with_Generative_Models.md)
7.  `CVPR` [Robust physical-world attacks on deep learning visual classification](./2018/Robust_physical_world_attacks_on_deep_learning_visual_classification.md)
8.  `ICLR` [Spatially Transformed Adversarial Examples](./2018/SPATIALLY_TRANSFORMED_ADVERSARIAL_EXAMPLES.md)
9.  `CVPR`[Boosting Adversarial Attacks With Momentum](./2018/Boosting_Adversarial_Attacks_With_Momentum.md)
10. `ICML` [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](./2018/Obfuscated_Gradients_Give_a_False_Sense_of_Security_Circumventing_Defenses_to_Adversarial_Examples.md) :thumbsup:
11. `CVPR` `UNIVERSAL` [Art of Singular Vectors and Universal Adversarial Perturbations]
12. `ARXIV` [Adversarial Spheres]
13. `ECCV` [Characterizing adversarial examples based on spatial consistency information for semantic segmentation]
14. `ARXIV` [Generating natural language adversarial examples]
15. `SP` [Audio adversarial examples: Targeted attacks on speech-to-text]
16. `ARXIV` [Adversarial attack on graph structured data]


### 2019
1. `CVPR` [Feature Space Perturbations Yield More Transferable Adversarial Examples](./2019/Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples.md)
2. `ICLR` [The Limitations of Adversarial Training and the Blind-Spot Attack](./2019/The_Limitations_of_Adversarial_Training_and_the_Blind-Spot_Attack.md)
3.  `ICLR` [Are adversarial examples inevitable?](./2019/Are_adversarial_examples_inevitable.md) :thought_balloon:
4.  `IEEE TEC` [One pixel attack for fooling deep neural networks](./2019/One_pixel_attack_for_fooling_deep_neural_networks.md)
5.  `ARXIV` [Generalizable Adversarial Attacks Using Generative Models](./2019/Generalizable_Adversarial_Attacks_Using_Generative_Models.md)
6.  `ICML` [NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks](./2019/NATTACK_Learning_the_Distributions_of_Adversarial_Examples_for_an_Improved_Black_Box_Attack_on_Deep_Neural_Networks.md):thought_balloon:
7.  `ARXIV` [SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing](./2019/SemanticAdv_Generating_Adversarial_Examples_via_Attribute_conditional_Image_Editing.md)
8.  `CVPR` [Rob-GAN: Generator, Discriminator, and Adversarial Attacker](./2019/Rob_GAN_Generator_Discriminator_and_Adversarial_Attacker.md)
9.  `ARXIV` [Cycle-Consistent Adversarial {GAN:} the integration of adversarial attack and defense](./2019/Cycle_Consistent_Adversarial_{GAN}_the_integration_of_adversarial_attack_and_defense.md)
10. `ARXIV` [Generating Realistic Unrestricted Adversarial Inputs using Dual-Objective {GAN} Training](./2019/Generating_Realistic_Unrestricted_Adversarial_Inputs_using_Dual_Objective_{GAN}_Training.md) :thought_balloon:
11. `ICCV` [Sparse and Imperceivable Adversarial Attacks](./2019/Sparse_and_Imperceivable_Adversarial_Attacks.md):thought_balloon:
12. `ARXIV` [Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions](2019/Perturbations_are_not_Enough_Generating_Adversarial_Examples_with_Spatial_Distortions.md)
13. `ARXIV` [Joint Adversarial Training: Incorporating both Spatial and Pixel Attacks](2019/Joint_Adversarial_Training_Incorporating_both_Spatial_and_Pixel_Attacks.md)
14. `IJCAI` [Transferable Adversarial Attacks for Image and Video Object Detection](./2019/Transferable_Adversarial_Attacks_for_Image_and_Video_Object_Detection.md)
15. `TPAMI` [Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations](./2019/Generalizable_Adversarial_Attacks_Using_Generative_Models.md)
16. `CVPR` [Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses](./2019/Decoupling_Direction_and_Norm_for_Efficient_Gradient_Based_L2_Adversarial_Attacks_and_Defenses.md)
17. `CVPR` [FDA: Feature Disruptive Attack]
18. `ARXIV` [SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations]
19. `CVPR` [SparseFool: a few pixels make a big difference]
20. `ICLR` [Adversarial Attacks on Graph Neural Networks via Meta Learning]
21. `NeurIPS` [Deep Leakage from Gradients]

### 2020
1. `ICLR` [Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking](./2020/Fooling_Detection_Alone_is_Not_Enough_Adversarial_Attack_against_Multiple_Object_Tracking.md):thought_balloon:
2. `ARXIV` [Sponge Examples: Energy-Latency Attacks on Neural Networks]
3. `ICML` [Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack]
4. `ICML` [Stronger and Faster Wasserstein Adversarial Attacks]
5. `CVPR` [QEBA: Query-EfÔ¨Åcient Boundary-Based Blackbox Attack]
6. `ECCV` [New Threats Against Object Detector with Non-local Block]


## Defence
### 2014
1. `ARXIV`  [Towards deep neural network architectures robust to adversarial examples](2014/Towards_deep_neural_network_architectures_robust_to_adversarial_examples.md)

### 2015
1. [Learning with a strong adversary]
2. [IMPROVING BACK-PROPAGATION BY ADDING AN ADVERSARIAL GRADIENT]
3. [Distributional Smoothing with Virtual Adversarial Training]


### 2016
1. `NIPS` [Robustness of classifiers: from adversarial to random noise](./2016/Robustness_of_classifiers_from_adversarial_to_random_noise.md) :thought_balloon:

### 2017
1. `ARXIV` [Countering Adversarial Images using Input Transformations](./2017/Countering_Adversarial_Images_using_Input_Transformations.md)
2. `ICCV` [SafetyNet: Detecting and Rejecting Adversarial Examples Robustly]
3. `Arxiv` [Detecting adversarial samples from artifacts](./2017/Detecting_Adversarial_Samples_from_Artifacts.md)
4. `ICLR` [On Detecting Adversarial Perturbations](./2017/On_Detecting_Adversarial_Perturbations.md) :thought_balloon:
5. `ASIA CCS` [Practical black-box attacks against machine learning]
6. `ARXIV` [The space of transferable adversarial examples]
7. `ICCV` [Adversarial Examples for Semantic Segmentation and Object Detection]

### 2018
1. `ICLR` [Defense-{GAN}: Protecting Classifiers Against Adversarial Attacks Using Generative Models](./2018/Defense-{GAN}_Protecting_Classifiers_Against_Adversarial_Attacks_Using_Generative_Models.md)
2. . `ICLR` [Ensemble Adversarial Training: Attacks and Defences](./2018/Ensemble_Adversarial_Training_Attacks_and_Defenses.md)
3.  `CVPR` [Defense Against Universal Adversarial Perturbations](./2018/Defense_Against_Universal_Adversarial_Perturbations.md)
4.  `CVPR` [Deflecting Adversarial Attacks With Pixel Deflection](./2018/Deflecting_Adversarial_Attacks_With_Pixel_Deflection.md)
5.  `TPAMI` [Virtual adversarial training: a regularization method for supervised and semi-supervised learning](./2018/Virtual_adversarial_training_a_regularization_method_for_supervised_and_semi_supervised_learning.md) :thought_balloon:
6.  `ARXIV` [Adversarial Logit Pairing](./2018/Adversarial_Logit_Pairing.md)
7.  `CVPR` [Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser](./2018/Defense_Against_Adversarial_Attacks_Using_High_Level_Representation_Guided_Denoiser.md)
8.  `ARXIV` [Evaluating and understanding the robustness of adversarial logit pairing](./2018/Evaluating_and_understanding_the_robustness_of_adversarial_logit_pairing.md)
9.  `CCS` [Machine Learning with Membership Privacy Using Adversarial Regularization](./2018/Machine_Learning_with_Membership_Privacy_Using_Adversarial_Regularization.md)
10. `ARXIV` [On the robustness of the cvpr 2018 white-box adversarial example defenses]
11. `ICLR` [Thermometer Encoding: One Hot Way To Resist Adversarial Examples]
12. `IJCAI` [Curriculum Adversarial Training]
13. `ICLR` [Countering Adversarial Images using Input Transformations]
14. `CVPR` [Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser]
15. `ICLR` [Towards Deep Learning Models Resistant to Adversarial Attacks]
16. `AAAI` [Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients]
17. `NIPS` [Adversarially robust generalization requires more data]
18. `ARXIV` [Is robustness the cost of accuracy? - {A} comprehensive study on the robustness of 18 deep image classification models.]
19. `ARXIV` [Robustness may be at odds with accuracy]

### 2019
1. `NIPS` [Adversarial Training and Robustness for Multiple Perturbations](./2019/Adversarial_Training_and_Robustness_for_Multiple_Perturbations.md)
2. `NIPS` [Adversarial Robustness through Local Linearization](./2019/Adversarial_Robustness_through_Local_Linearization.md)
3.  `CVPR` [Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples](./2019/Retrieval_Augmented_Convolutional_Neural_Networks_against_Adversarial_Examples.md)
4.  `CVPR` [Feature Denoising for Improving Adversarial Robustness](./2019/Feature_Denoising_for_Improving_Adversarial_Robustness.md)
5.  `NEURIPS` [A New Defense Against Adversarial Images: Turning a Weakness into a Strength](./2019/A_New_Defense_Against_Adversarial_Images_Turning_a_Weakness_into_a_Strength.md)
6.  `ICML` [Interpreting Adversarially Trained Convolutional Neural Networks](./2019/Interpreting_Adversarially_Trained_Convolutional_Neural_Networks.md)
7.  `ICLR` [Robustness May Be at Odds with Accuracy](./2019/Robustness_May_Be_at_Odds_with_Accuracy.md):thought_balloon:
8.  `IJCAI` [Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss](./2019/Improving_the_Robustness_of_Deep_Neural_Networks_via_Adversarial_Training_with_Triplet_Loss.md)
9.  `ICML` [Adversarial Examples Are a Natural Consequence of Test Error in Noise](./2019/Adversarial_Examples_Are_a_Natural_Consequence_of_Test_Error_in_Noise.md):thought_balloon:
10. `ICML` [On the Connection Between Adversarial Robustness and Saliency Map Interpretability](./2019/On_the_Connection_Between_Adversarial_Robustness_and_Saliency_Map_Interpretability.md)
11. `NeurIPS` [Metric Learning for Adversarial Robustness](./2019/Metric_Learning_for_Adversarial_Robustness.md)
12. `ARXIV` [Defending Adversarial Attacks by Correcting logits](./2019/Defending_Adversarial_Attacks_by_Correcting_logits.md)
13. `ICCV` [Adversarial Learning With Margin-Based Triplet Embedding Regularization](./2019/Adversarial_Learning_With_Margin_Based_Triplet_Embedding_Regularization.md)
14. `ICCV` [CIIDefence: Defeating Adversarial Attacks by Fusing Class-Specific Image Inpainting and Image Denoising](./2019/CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class_Specific_Image_Inpainting_and_Image_Denoising.md)
15. `NIPS` [Adversarial Examples Are Not Bugs, They Are Features](./2019/Adversarial_Examples_Are_Not_Bugs_They_Are_Features.md)
16. `ICML` [Using Pre-Training Can Improve Model Robustness and Uncertainty](./2019/Using_Pre_Training_Can_Improve_Model_Robustness_and_Uncertainty.md)
17. `NIPS` [Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training](./2019/Defense_Against_Adversarial_Attacks_Using_Feature_Scattering_based_Adversarial_Training.md):thought_balloon:
18. `ICCV` [Improving Adversarial Robustness via Guided Complement Entropy](/2019/Improving_Adversarial_Robustness_via_Guided_Complement_Entropy.md)
19. `NIPS` [Robust Attribution Regularization](./2019/Robust_Attribution_Regularization.md) :thought_balloon:
20. `NIPS` [Are Labels Required for Improving Adversarial Robustness?](./2019/Are_Labels_Required_for_Improving_Adversarial_Robustness.md)
21. `ICLR` [Theoretically Principled Trade-off between Robustness and Accuracy](./2019/Theoretically_Principled_Trade_off_between_Robustness_and_Accuracy.md)
22. `CVPR` [Adversarial defense by stratified convolutional sparse coding]
23. `ICML` [On the Convergence and Robustness of Adversarial Training]
24. `CVPR` [Robustness via Curvature Regularization, and Vice Versa]
25. `CVPR` [ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples]
26. `ICML` [Improving Adversarial Robustness via Promoting Ensemble Diversity]
27. `ICML` [Towards the first adversarially robust neural network model on {MNIST}]
28. `NIPS` [Unlabeled Data Improves Adversarial Robustness]
29. `ICCV` [Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks]
30. `ICML` [Using Pre-Training Can Improve Model Robustness and Uncertainty]
31. `ARXIV` [Improving adversarial robustness of ensembles with diversity training]
32. `ICML` [Adversarial Robustness Against the Union of Multiple Perturbation Models]
33. `CVPR` [Robustness via Curvature Regularization, and Vice Versa]
34. `NIPS` [Robustness to Adversarial Perturbations in Learning from Incomplete Data]
35. `ICML` [Improving Adversarial Robustness via Promoting Ensemble Diversity]
36. `NIPS` [Adversarial Robustness through Local Linearization]
37. `ARXIV` [Adversarial training can hurt generalization]
38. `NIPS` [Adversarial training for free!]
39. `ICLR` [Improving the generalization of adversarial training with domain adaptation]
40. `CVPR` [Disentangling Adversarial Robustness and Generalization]
41. `NIPS` [Adversarial Training and Robustness for Multiple Perturbations]
42. `ICCV` [Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks]
43. `ICML` [On the Convergence and Robustness of Adversarial Training]
44. `ICML` [Rademacher Complexity for Adversarially Robust Generalization]
45. `ARXIV` [Adversarially Robust Generalization Just Requires More Unlabeled Data]
46. `ARXIV` [You only propagate once: Accelerating adversarial training via
  maximal principle]


### 2020
1. `ICLR` [Jacobian Adversarially Regularized Networks for Robustness](./2020/Jacobian_Adversarially_Regularized_Networks_for_Robustness.md)
2. `CVPR` [What it Thinks is Important is Important: Robustness Transfers through Input Gradients](./2020/What_it_Thinks_is_Important_is_Important_Robustness_Transfers_through_Input_Gradients.md)
3.  `ICLR` [Adversarially Robust Representations with Smooth Encoders](2020/Adversarially_Robust_Representations_with_Smooth_Encoders.md) :thought_balloon:
4.  `ARXIV` [Heat and Blur: An Effective and Fast Defense Against Adversarial Examples](./2020/Heat_and_Blur_An_Effective_and_Fast_Defense_Against_Adversarial_Examples.md)
5.  `ICML` [Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference](./2020/Triple_Wins_Boosting_Accuracy_Robustness_and_Efficiency_Together_by_Enabling_Input_Adaptive_Inference.md)
6.  `CVPR` [Wavelet Integrated CNNs for Noise-Robust Image Classification](./2020/Wavelet_Integrated_CNNs_for_Noise_Robust_Image_Classification.md)
7.  `ARXIV` [Deflecting Adversarial Attacks](./2020/Deflecting_Adversarial_Attacks.md)
8.  `ICLR` [Robust Local Features for Improving the Generalization of Adversarial Training](./2020/Robust_Local_Features_for_Improving_the_Generalization_of_Adversarial_Training.md)
9.  `ICLR` [Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier](./2020/Enhancing_Transformation_Based_Defenses_Against_Adversarial_Attacks_with_a_Distribution_Classifier.md)
10. `CVPR` [A Self-supervised Approach for Adversarial Robustness](./2020/A_Self_supervised_Approach_for_Adversarial_Robustness.md)
11. `ICLR` [Improving Adversarial Robustness Requires Revisiting Misclassified Examples](./2019/Improving_the_Robustness_of_Deep_Neural_Networks_via_Adversarial_Training_with_Triplet_Loss.md) :thumbsup:
12. `ARXIV` [Manifold regularization for adversarial robustness](2020/Manifold_regularization_for_adversarial_robustness.md)
13. `NeurIPS` [DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles](./2020/DVERGE_Diversifying_Vulnerabilities_for_Enhanced_Robust_Generation_of_Ensembles.md)
14. `ARXIV` [A Closer Look at Accuracy vs. Robustness](./2020/A_Closer_Look_at_Accuracy_vs_Robustness.md)
15. `NeurIPS` [Energy-based Out-of-distribution Detection](./2020/Energy_based_Out_of_distribution_Detection.md)
16. `ARXIV` [Out-of-Distribution Generalization via Risk Extrapolation (REx)](./2020/Out_of_Distribution_Generalization_via_Risk_Extrapolation.md)
17. `CVPR` [Adversarial Examples Improve Image Recognition](2020\Adversarial_Examples_Improve_Image_Recognition.md)
18. `ICML` [Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks] :thumbsup:
19. `ICML` [Efficiently Learning Adversarially Robust Halfspaces with Noise]
20. `ICML` [Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability]
21. `ICML` [Friendly Adversarial Training: Attacks Which Do Not Kill Training Make Adversarial Learning Stronger]
22. `ICML` [Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization] :thumbsup:
23. `ICML` [Overfitting in adversarially robust deep learning] :thumbsup:
24. `ICML` [Proper Network Interpretability Helps Adversarial Robustness in Classification]
25. `ICML` [Randomization matters How to defend against strong adversarial attacks]
26. `ICML` [Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks]
27. `ICML` [Towards Understanding the Regularization of Adversarial Robustness on Neural Networks]
28. `CVPR` [Defending Against Universal Attacks Through Selective Feature Regeneration]
29. `ARXIV` [Understanding and improving fast adversarial training]
30. `ARXIV` [Cat: Customized adversarial training for improved robustness]
31. `ICLR` [MMA Training: Direct Input Space Margin Maximization through Adversarial Training]
32. `ARXIV` [Bridging the performance gap between fgsm and pgd adversarial training]
33. `CVPR` [Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization]
34. `ARXIV` [Towards understanding fast adversarial training]
35. `ARXIV` [Overfitting in adversarially robust deep learning]
36. `ICLR` [Robust local features for improving the generalization of adversarial training]
37. `ICML` [Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks]
38. `ARXIV` [Regularizers for single-step adversarial training]
39. `CVPR` [Single-step adversarial training with dropout scheduling]
40. `ICLR` [Improving Adversarial Robustness Requires Revisiting Misclassified Examples]
41. `ARXIV` [Fast is better than free: Revisiting adversarial training.]
42. `ARXIV` [On the Generalization Properties of Adversarial Training]
43. `ARXIV` [A closer look at accuracy vs. robustness]


### 2021
1. `ARXIV` [On the Limitations of Denoising Strategies as Adversarial Defenses](./2021/On_the_Limitations_of_Denoising_Strategies_as_Adversarial_Defenses.md)
2. `AAAI` [Understanding catastrophic overfitting in single-step adversarial training]
3. `ICLR` [Bag of tricks for adversarial training]

## 4th-Class
1. `ICCV 2017` [CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training](./2017/CVAE-GAN_Fine-Grained_Image_Generation_Through_Asymmetric_Training.md)
2. `ICML 2016` [Autoencoding beyond pixels using a learned similarity metric](./2016/Autoencoding_beyond_pixels_using_a_learned_similarity_metric.md) 
3. `ARXIV 2019` [Natural Adversarial Examples](./2019/Natural_Adversarial_Examples.md)
4. `ICML 2017` [Conditional Image Synthesis with Auxiliary Classifier {GAN}s](./2017/Conditional_Image_Synthesis_with_Auxiliary_Classifier_GANs.md)
5. `ICCV 2019` [SinGAN: Learning a Generative Model From a Single Natural Image](./2019/SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image.md)
6. `ICLR 2020` [Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks](./2020/Robust_And_Interpretable_Blind_Image_Denoising_Via_Bias_Free_Convolutional_Neural_Networks.md)
7. `ICLR 2020` [Pay Attention to Features, Transfer Learn Faster CNNs](./2020/Pay_Attention_to_Features_Transfer_Learn_Faster_CNNs.md)
8. `ICLR 2020` [On Robustness of Neural Ordinary Differential Equations](./2020/On_Robustness_of_Neural_Ordinary_Differential_Equations.md)
9. `ICCV 2019` [Real Image Denoising With Feature Attention](./2019/Real_Image_Denoising_With_Feature_Attention.md)
10. `ICLR 2018` [Multi-Scale Dense Networks for Resource Efficient Image Classification](./2018/Multi_Scale_Dense_Networks_for_Resource_Efficient_Image_Classification.md)
11. `ARXIV 2019` [Rethinking Data Augmentation: Self-Supervision and Self-Distillation](2019/Rethinking_Data_Augmentation_Self_Supervision_and_Self_Distillation.md)
12. `ICCV 2019` [Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation](./2019/Be_Your_Own_Teacher_Improve%20the_Performance_of_Convolutional_Neural_Networks_via_Self_Distillation.md)
13. `ARXIV 2019` [Adversarially Robust Distillation](./2019/Adversarially_Robust_Distillation.md)
14. `ARXIV 2019` [Knowledge Distillation from Internal Representations](./2019/Knowledge_Distillation_from_Internal_Representations.md)
15. `ICLR 2020` [Contrastive Representation Distillation](./2020/Contrastive_Representation_Distillation.md) :thought_balloon:
16. `NIPS 2018` [Faster Neural Networks Straight from JPEG](./2018/Faster_Neural_Networks_Straight_from_JPEG.md)
17. `ARXIV 2019` [A Closer Look at Double Backpropagation](./2019/A_Closer_Look_at_Double_Backpropagation.md):thought_balloon:
18. `CVPR 2016` [Learning Deep Features for Discriminative Localization](./2016/Learning_Deep_Features_for_Discriminative_Localization.md)
19. `ICML 2019` [Noise2Self: Blind Denoising by Self-Supervision](./2019/Noise2Self_Blind_Denoising_by_Self_Supervision.md)
20. `ARXIV 2020` [Supervised Contrastive Learning](./2020/Supervised_Contrastive_Learning.md)
21. `CVPR 2020` [High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks](./2020/High_Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks.md)
22. `NIPS 2017` [Counterfactual Fairness]
23. `ARXIV 2020` [An Adversarial Approach for Explaining the Predictions of Deep Neural Networks]
24. `CVPR 2014` [Rich feature hierarchies for accurate object detection and semantic segmentation]
25. `ICLR 2018` [Spectral Normalization for Generative Adversarial Networks]
26. `NIPS 2018` [MetaGAN: An Adversarial Approach to Few-Shot Learning]
27. `ARXIV 2019` [Breaking the cycle -- Colleagues are all you need]
28. `ARXIV 2019` [LOGAN: Latent Optimisation for Generative Adversarial Networks]
29. `ICML 2020` [Margin-aware Adversarial Domain Adaptation with Optimal Transport] 
30. `ICML 2020` [Representation Learning Using Adversarially-Contrastive Optimal Transport]
31. [Free Lunch for Few-shot Learning: Distribution Calibration]


## Links
- [Adversarial Machine Learning Reading List](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html) by [Nicholas Carlini](https://nicholas.carlini.com)
- [A Complete List of All (arXiv) Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) by [Nicholas Carlini](https://nicholas.carlini.com) **Stay Tuned** 
